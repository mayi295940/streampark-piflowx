/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package cn.piflow.bundle.spark.neo4j

import cn.piflow._
import cn.piflow.conf.{ConfigurableStop, Language, Port, StopGroup}
import cn.piflow.conf.bean.PropertyDescriptor
import cn.piflow.conf.util.{ImageUtil, MapUtil}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}
import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
import org.neo4j.driver.v1._

import scala.collection.mutable.ArrayBuffer

class HiveToNeo4j extends ConfigurableStop[DataFrame] {

  override val authorEmail: String = "anhong12@cnic.cn"
  override val description: String = "Hive to Neo4j"
  override val inportList: List[String] = List(Port.DefaultPort)
  override val outportList: List[String] = List(Port.DefaultPort)

  var hiveQL: String = _
  var hdfsDirPath: String = _
  var hdfsUrl: String = _
  var fileName: String = _
  var delimiter: String = _
  var header: Boolean = _

  var pathARR: ArrayBuffer[String] = ArrayBuffer()
  var oldFilePath: String = _

  var neo4j_Url: String = _
  var userName: String = _
  var password: String = _
  var cypher: String = ""

  override def perform(
      in: JobInputStream[DataFrame],
      out: JobOutputStream[DataFrame],
      pec: JobContext[DataFrame]): Unit = {

    // selectHiveQL-----------------------------
    val spark = pec.get[SparkSession]()
    println("Select HiveQL started !!!!!!!!!!!!!!!!!!!")
    import spark.sql
    val df = sql(hiveQL)
    println("Select HiveQL done !!!!!!!!!!!!!!!!!!!!!")

    println("All Fields cleaning started !!!!!!!!!!!!!!!!!!!!!!!!!!!f")
    df.createOrReplaceTempView("temp")

    // clean authors
    spark.sqlContext.udf.register(
      "cleanFields",
      (fieldStr: String) => {
        if (fieldStr == null) null
        else fieldStr.replaceAll("\"", "")
      })

    val fieldNames: Array[String] = df.schema.fieldNames
    val sqlStringBuilder = new StringBuilder
    for (i <- fieldNames.indices) {
      sqlStringBuilder.append("cleanFields(" + fieldNames(i) + ") as " + fieldNames(i) + ",")
    }

    val sqlString = sqlStringBuilder.dropRight(1).toString()
    val frame: DataFrame = spark.sql("select   " + sqlString + "   from temp")

    println("All Fields Cleaned!!!!!!!!!!!!!!!!!!!!")

    // frame   now is cleaned data
    // CSVSaveToHDFS----------------------------------

    println("save csv file started !!!!!!!!!!!!!!!!!!!!!!!!!!!")
    val hdfsDir = hdfsUrl + hdfsDirPath

    val config = new Configuration()
    config.set("fs.defaultFS", hdfsUrl)
    val fs = FileSystem.get(config)

    frame
      .repartition(1)
      .write
      .format("csv")
      .mode(SaveMode.Overwrite)
      .option("header", header)
      .option("delimiter", delimiter)
      .save(hdfsDir)

    iterationFile(hdfsDir)

    val oldPath = new Path(oldFilePath)
    val newPath = new Path(hdfsDir + Constants.SINGLE_SLASH + fileName)
    fs.rename(oldPath, newPath)

    println("csv file has been written to HDFS!!!!!!!!!!!!!!!!!!!! ")

    // RunCypherLoadCSV----------------------------------------
    println("run cypher !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    val cqls: Array[String] = cypher.split(";").map(x => x.trim)
    val driver: Driver = GraphDatabase.driver(neo4j_Url, AuthTokens.basic(userName, password))
    var session: Session = null

    try {
      session = driver.session()
      cqls.foreach(eachCql => {
        session.run(eachCql)
      })
    } finally {
      session.close()
      driver.close()
    }

    println("load csv done!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

  }

  // recursively traverse the folder
  def iterationFile(path: String): Unit = {

    val config = new Configuration()
    config.set("fs.defaultFS", hdfsUrl)
    val fs = FileSystem.get(config)

    val listf = new Path(path)

    val statuses: Array[FileStatus] = fs.listStatus(listf)

    for (f <- statuses) {
      val fsPath = f.getPath.toString
      if (f.isDirectory) {
        //        pathARR += fsPath
        iterationFile(fsPath)
      } else {
        if (f.getPath.toString.contains("part")) {
          pathARR += hdfsUrl + hdfsDirPath + Constants.SINGLE_SLASH + fileName
          oldFilePath = f.getPath.toString
        }
      }
    }
  }

  override def setProperties(map: Map[String, Any]): Unit = {
    hiveQL = MapUtil.get(map, "hiveQL").asInstanceOf[String]
    hdfsUrl = MapUtil.get(map, key = "hdfsUrl").asInstanceOf[String]
    hdfsDirPath = MapUtil.get(map, key = "hdfsDirPath").asInstanceOf[String]
    fileName = MapUtil.get(map, key = "fileName").asInstanceOf[String]
    delimiter = MapUtil.get(map, key = "delimiter").asInstanceOf[String]
    header = MapUtil.get(map, "header").asInstanceOf[String].toBoolean
    neo4j_Url = MapUtil.get(map, "neo4j_Url").asInstanceOf[String]
    userName = MapUtil.get(map, "userName").asInstanceOf[String]
    password = MapUtil.get(map, "password").asInstanceOf[String]
    cypher = MapUtil.get(map, "cypher").asInstanceOf[String]
  }

  override def getPropertyDescriptor(): List[PropertyDescriptor] = {
    var descriptor: List[PropertyDescriptor] = List()

    val hiveQL = new PropertyDescriptor()
      .name("hiveQL")
      .displayName("HiveQL")
      .description("SQL statement saved from hive to neo4j")
      .defaultValue("")
      .required(true)
      .language(Language.Sql)
      .example("select * from test.user1")

    val hdfsDirPath = new PropertyDescriptor()
      .name("hdfsDirPath")
      .displayName("HdfsDirPath")
      .description("Path saved to hdfs")
      .defaultValue("")
      .required(true)
      .example("/piflow-CSV-of-Neo4j/xxxxx")

    val hdfsUrl = new PropertyDescriptor()
      .name("hdfsUrl")
      .displayName("HdfsUrl")
      .description("The Url of hdfs")
      .defaultValue("")
      .required(true)
      .example("hdfs://127.0.0.1:8020")

    val fileName = new PropertyDescriptor()
      .name("fileName")
      .displayName("FileName")
      .description("Csv file name saved to hdfs")
      .defaultValue("")
      .required(true)
      .example("test.csv")

    val delimiter = new PropertyDescriptor()
      .name("delimiter")
      .displayName("Delimiter")
      .description("Set separator from csv file")
      .defaultValue("Â¤")
      .required(true)
      .example(",")

    // header
    val header = new PropertyDescriptor()
      .name("header")
      .displayName("Header")
      .description("Whether the csv file have header or not")
      .defaultValue("true")
      .allowableValues(Set("true", "false"))
      .required(true)
      .example("true")

    val neo4j_Url = new PropertyDescriptor()
      .name("neo4j_Url")
      .displayName("Neo4j_Url")
      .description("The url of neo4j")
      .defaultValue("")
      .required(true)
      .example("bolt://127.0.0.1:7687")

    val userName = new PropertyDescriptor()
      .name("userName")
      .displayName("UserName")
      .description("The user of neo4j")
      .defaultValue("neo4j")
      .required(true)
      .example("neo4j")

    val password = new PropertyDescriptor()
      .name("password")
      .displayName("Password")
      .description("The password of neo4j")
      .defaultValue("")
      .required(true)
      .example("123456")

    val cypher = new PropertyDescriptor()
      .name("cypher")
      .displayName("Cypher")
      .description("Cypher statement to import csv file")
      .defaultValue("")
      .required(true)
      .example("USING PERIODIC COMMIT 10 LOAD CSV WITH HEADERS FROM 'http://127.0.0.1:50070/webhdfs/v1/test/user.csv?op=OPEN' AS line FIELDTERMINATOR ',' CREATE (n:user{userid:line.id,username:line.name,userscore:line.score,userschool:line.school,userclass:line.class})")

    descriptor = hiveQL :: descriptor
    descriptor = hdfsUrl :: descriptor
    descriptor = hdfsDirPath :: descriptor
    descriptor = fileName :: descriptor
    descriptor = header :: descriptor
    descriptor = delimiter :: descriptor

    descriptor = neo4j_Url :: descriptor
    descriptor = userName :: descriptor
    descriptor = password :: descriptor
    descriptor = cypher :: descriptor

    descriptor
  }

  override def getIcon(): Array[Byte] = {
    ImageUtil.getImage("icon/neo4j/HiveToNeo4j.png")
  }

  override def getGroup(): List[String] = {
    List(StopGroup.Neo4jGroup)
  }

  override def initialize(ctx: ProcessContext[DataFrame]): Unit = {}

  override def getEngineType: String = Constants.ENGIN_SPARK

}
